# 神經網路
在電腦領域，神經網路是指一種模擬神經系統所設計出來的程式，用來模擬人類視覺、聽覺等等智慧行為的原理，企圖讓電腦可以具有人類智慧的一種方法。<br>
但神經網路程式不需模擬「細胞膜、粒線體、核醣體」等複雜的結構，因為學電腦的人可以透過「抽象化」，將上述的神經細胞結構簡化成下圖的樣子。<br>
![](images/Ncell.png)<br>
```
a1~an為輸入向量的各個分量
w1~wn為神經元各個突觸的權值
b為偏置
f為傳遞函式，通常為非線性函式。
t為神經元輸出
```
上圖對應的數學公式如下：<br>
![](images/cell.jpg)<br>
# 從微分到梯度下降法
## 微分
[微分 程式碼](diff.py)
```
執行結果:
C:\Users\user\Desktop\ai\ai109b\筆記\0422>python diff.py
diff(f,2)= 12.006000999997823
```
[計算e的程式碼](e.py)
```
執行結果:
C:\Users\user\Desktop\ai\ai109b\筆記\0422>python e.py
n= 100.0 e(n)= 2.7048138294215285
n= 200.0 e(n)= 2.711517122929317
n= 300.0 e(n)= 2.7137651579427837
n= 400.0 e(n)= 2.7148917443812293
n= 500.0 e(n)= 2.715568520651728
(中間省略)
n= 9600.0 e(n)= 2.718140264795318
n= 9700.0 e(n)= 2.718141724076723
n= 9800.0 e(n)= 2.718143153583405
n= 9900.0 e(n)= 2.718144554210053
n= 10000.0 e(n)= 2.7181459268249255
```
## 梯度下降法
深度學習(Deep Learning)是人工智慧領域當紅的技術，說穿了其實就是原本的《神經網路》(Neural Network) ，不過由於加上了一些新的模型 (像是捲積神經網路 CNN, 循環神經網路 RNN 與生成對抗網路 GAN)，還有在神經網路的層數上加深很多，從以往的 3-4 層，提升到了十幾層，甚至上百層，於是我們給這些新一代的《神經網路》技術一個統稱，那就是《深度學習》。<br>
雖然《深度學習》的神經網路層數變多了，《網路模型》也多了一些，但是背後的學習算法和運作原理並沒有多大改變，仍然是以《梯度下降》(Gradient Descendent) 和《反傳遞算法》(Back Propagation) 為主。<br>
### 梯度
《梯度》的數學定義如下：<br>
![](images/gradient0.jpg)<br>
梯度就是斜率最大的那個方向<br>
而梯度下降法就是朝著逆梯度的方向走，就可以不斷下降，直到到達梯度為 0 的點 (斜率最大的方向仍然是斜率為零)，也就是區域的最低點。<br>
下圖中往箭頭方向則梯度大，反之則小<br>
![](images/gradient.jpg)<br>
當要用程式計算梯度時，我們可以從梯度的定義中看到他的基本元素，也就是「偏微分」。<br>
因此我們先用程式寫出計算偏微分的程式:
```
# 函數 f 對變數 k 的偏微分: df / dk
def df(f, p, k, step=0.01):
    p1 = p.copy()
    p1[k] = p[k]+step
    return (f(p1) - f(p)) / step

利用以下指令就能夠計算出 f(x,y) 在 (1,1) 這點的偏導數
p = [1.0, 1.0]
print('nn.df(f, p, 0) = ', nn.df(f, p, 0))
```
能夠算偏微分之後，我們就能夠利用[npGradient.py](npGradient.py)算出梯度了
在學會計算梯度後，就可以開始使用梯度下降法了!
### 梯度下降法
利用[gdTest](gdTest.py)可以求 x*x + y*y 的最低點<br>
下圖為x*x + y*y的圖形:<br>
![](images/xx+yy.jpg)<br>
在執行程式後可以看到他最後找到了(0,0)這個點<br>
```
執行結果:
1:p=[1.0, 3.0] f(p)=10.000 gp=[2.009999999999934, 6.009999999999849] glen=6.33721
2:p=[0.9799 2.9399] f(p)=9.603 gp=[1.9698 5.8898] glen=6.21046
3:p=[0.960202 2.881002] f(p)=9.222 gp=[1.930404 5.772004] glen=6.08625
4:p=[0.94089796 2.82328196] f(p)=8.856 gp=[1.89179592 5.65656392] glen=5.96453
5:p=[0.92198    2.76671632] f(p)=8.505 gp=[1.85396    5.54343264] glen=5.84524
(中間省略)
658:p=[-0.00499827 -0.00499483] f(p)=0.000 gp=[3.45722613e-06 1.03372781e-05] glen=0.00001
659:p=[-0.00499831 -0.00499493] f(p)=0.000 gp=[3.38808161e-06 1.01305326e-05] glen=0.00001
660:p=[-0.00499834 -0.00499504] f(p)=0.000 gp=[3.32031998e-06 9.92792193e-06] glen=0.00001
661:p=[-0.00499837 -0.00499514] f(p)=0.000 gp=[3.25391358e-06 9.72936349e-06] glen=0.00001
662:p=[-0.00499841 -0.00499523] f(p)=0.000 gp=[3.18883531e-06 9.53477622e-06] glen=0.00001
663:p=[-0.00499844 -0.00499533] f(p)=0.000 gp=[3.1250586e-06 9.3440807e-06] glen=0.00001
```